{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cmTr9Lgzr-O"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \"torch==2.1.2\" tensorboard\n",
        "!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISseX4ELz0Z9"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
        "!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3luZu392-KFw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np             # Library Arrays Evaluation\n",
        "import pandas as pd            # Library for data manipulation\n",
        "import os\n",
        "from tqdm import tqdm          # Library for monitoring the progress of loops and tasks.\n",
        "import bitsandbytes as bnb     \n",
        "import torch                   # Library for natural language processing\n",
        "from datasets import Dataset   # Hugging Face library that provides access to various datasets\n",
        "\n",
        "from peft import PeftConfig    # Finetune Model training and evaluation of models\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from sklearn.metrics import (accuracy_score,    # Library providing simple and efficient tools for predictive data analysis and classification.\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FldhLRukz2e4"
      },
      "outputs": [],
      "source": [
        "# Print PyTorch version and set device to CUDA if available, otherwise to CPU\n",
        "print(f\"Using PyTorch version {torch.__version__}\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define file path for data\n",
        "filename = \"../input/sentiment-analysis/all-data.csv\"\n",
        "\n",
        "# Read CSV file into a DataFrame, specifying column names and encoding\n",
        "df = pd.read_csv(filename, names=[\"sentiment\", \"text\"], encoding=\"utf-8\")\n",
        "\n",
        "# Split data into train, test, and evaluation sets for each sentiment category\n",
        "X_train = []\n",
        "X_test = []\n",
        "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
        "    train, test = train_test_split(df[df.sentiment == sentiment], train_size=300, test_size=300, random_state=42)\n",
        "    X_train.append(train)\n",
        "    X_test.append(test)\n",
        "\n",
        "# Concatenate and shuffle train and test sets\n",
        "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
        "X_test = pd.concat(X_test)\n",
        "\n",
        "# Select evaluation data\n",
        "eval_idx = [idx for idx in df.index if idx not in list(train.index) + list(test.index)]\n",
        "X_eval = df[df.index.isin(eval_idx)]\n",
        "X_eval = (X_eval.groupby('sentiment', group_keys=False)\n",
        "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "\n",
        "# Define functions to generate prompts for training and evaluation data\n",
        "def generate_prompt(data_point):\n",
        "    return f\"Analyze the sentiment of the news headline enclosed in square brackets, determine if it is positive, neutral, or negative, and return the answer as the corresponding sentiment label \\\"positive\\\" or \\\"neutral\\\" or \\\"negative\\\".\\n\\n[{data_point['text']}] = {data_point['sentiment']}\"\n",
        "\n",
        "# Apply prompt generation functions to train, evaluation, and test data\n",
        "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), columns=[\"text\"])\n",
        "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), columns=[\"text\"])\n",
        "y_true = X_test.sentiment\n",
        "X_test = pd.DataFrame(X_test.apply(generate_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "# Create datasets from pandas DataFrames\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFZrcjZO-Hgj"
      },
      "outputs": [],
      "source": [
        "# Define evaluation function\n",
        "def evaluate(y_true, y_pred):\n",
        "    # Define labels and mapping for sentiment categories\n",
        "    labels = ['positive', 'neutral', 'negative']\n",
        "    mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "\n",
        "    # Define function to map sentiment labels to numerical values\n",
        "    def map_func(x):\n",
        "        return mapping.get(x, 1)\n",
        "\n",
        "    # Map true and predicted labels to numerical values\n",
        "    y_true = np.vectorize(map_func)(y_true)\n",
        "    y_pred = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generate accuracy report for each label\n",
        "    unique_labels = set(y_true)\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n",
        "        label_y_true = [y_true[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred[i] for i in label_indices]\n",
        "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        print(f'Accuracy for label {label}: {label_accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "# Define model and tokenizer configurations\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "compute_dtype = getattr(torch)\n",
        "\n",
        "\n",
        "# Define configuration for BitsAndBytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model with specified configurations\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device,\n",
        "    torch_dtype=compute_dtype,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "# Load tokenizer and set padding configurations\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Setup chat format for the model\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO6EnMNK-Dfh"
      },
      "outputs": [],
      "source": [
        "def predict(test, model, tokenizer):\n",
        "    y_pred = []\n",
        "    for i in tqdm(range(len(test))):\n",
        "        prompt = test.iloc[i][\"text\"]\n",
        "        pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens=1,\n",
        "                        temperature=0.0,\n",
        "                       )\n",
        "        result = pipe(prompt)\n",
        "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
        "        if \"positive\" in answer:\n",
        "            y_pred.append(\"positive\")\n",
        "        elif \"negative\" in answer:\n",
        "            y_pred.append(\"negative\")\n",
        "        elif \"neutral\" in answer:\n",
        "            y_pred.append(\"neutral\")\n",
        "        else:\n",
        "            y_pred.append(\"none\")\n",
        "    return y_pred\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = predict(X_test, model, tokenizer)\n",
        "\n",
        "# Evaluate predictions\n",
        "evaluate(y_true, y_pred)\n",
        "\n",
        "# Define output directory for trained weights\n",
        "output_dir = \"trained_weights\"\n",
        "\n",
        "# Configure PEFT model\n",
        "peft_config = PEFT_config(\n",
        "    peft_alpha=16,\n",
        "    peft_dropout=0.1,\n",
        "    r=64,\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=0,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=1024,\n",
        "    packing=False,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model and tokenizer\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "\n",
        "# Clean up memory\n",
        "import gc\n",
        "del [model, tokenizer, peft_config, trainer, train_data, eval_data]\n",
        "del [training_arguments, SFTTrainer, BitsAndBytesConfig]\n",
        "\n",
        "# Clear GPU memory\n",
        "for _ in range(100):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Load fine-tuned model using AutoPeftModelForCausalLM\n",
        "finetuned_model_dir = \"./trained_weights/\"\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "\n",
        "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "     finetuned_model_dir,\n",
        "     torch_dtype=compute_dtype,\n",
        "     return_dict=False,\n",
        "     low_cpu_mem_usage=True,\n",
        "     device_map=device,\n",
        ")\n",
        "\n",
        "# Merge and unload model for efficient inference\n",
        "merged_model = finetuned_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
        "tokenizer.save_pretrained(\"./merged_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IbWuASC3REP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
